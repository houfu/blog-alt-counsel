# Pitch: Post 2 - "Building data.zeeker.sg: What Worked and What I'd Do Differently"

data.zeeker.sg worked technically - 100% uptime, fast queries, clean API, quality data - but some technical choices made the labor unsustainably high while others were exactly right. This post documents what the next legal data infrastructure project should copy (Datasette + SQLite for instant APIs, FTS5 for search, strict metadata standards, CC-BY-4.0 licensing) and what it should avoid (manual curation at scale, multiple data sources before proving single-source value, planning features that won't get built). The biggest lesson: AI-assisted summarization with human review could have cut per-article time from 30-45 minutes to 10-15 minutes - saving 100+ hours over 346 articles. I'm writing this partly to document what I learned, and partly as prep work for the SMU Centre for Digital Law event on Nov 18 where they're launching their legal database project - by crystallizing my technical insights, I can engage meaningfully and maybe help them avoid my expensive mistakes. Whether data.zeeker.sg continues or not, these lessons are the blueprint for anyone building similar infrastructure.
