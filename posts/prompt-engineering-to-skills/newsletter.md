---
title: "Why Prompt Engineering Felt Wrong (And What Skills Changed)"
type: newsletter
status: draft
---

# Why Prompt Engineering Felt Wrong (And What Skills Changed)

Standing room only at TechLawFest 2025. Everyone was learning COSTAR and GCES—frameworks for writing better prompts. I sat there feeling anxious. Not because I was struggling to keep up, but because I don't use any of this.

I use AI every day. Big things. Real work. But I've never typed "COSTAR" into a prompt in my life.

The anxious voice in my head kept asking: "Am I doing this wrong? Should I be learning these frameworks like everyone else?"

Turns out, that same month—September 2025—the tools changed underneath all of us.

## The Year Before

2024 was my rock bottom with prompt engineering. I entered a prompt writing competition. Spent weeks crafting a 3-page prompt for generating M&A term sheets. Six major sections. Step-by-step instructions. GCES framework. Mind-blowing formatting.

I was so proud of the detail. So convinced it would showcase what I knew.

I got disqualified for missing the deadline. Spent too much time perfecting it.

That 3-page prompt felt excessive even then. But I thought that was how you were supposed to do it. Write the perfect prompt. Specify every step. Make it comprehensive.

## What Changed in September

After TechLawFest, I started following different people. Simon Willison was documenting something new—agents that work in loops, not one-shot prompts. I found GitHub repos exploring these patterns.

I connected it to the problems I'd been facing. The repetition. The iteration. The feeling that I was re-solving the same problems over and over.

And I realized: I don't need to write better prompts. I need reusable systems.

Now I have a 45-line skill that does what that 3-page prompt tried to do. But instead of writing it fresh each time, the skill works every time. No iteration. No remembering the "right" format. I wrote it once. It runs autonomously.

That's the shift. Not "prompt better" but "build systems."

## If You Felt Like Prompt Engineering Didn't Fit

Here's what I learned: If you felt like prompt engineering frameworks didn't fit your work, your instinct was right.

Not because prompt engineering is wrong—it still works for chat tools like ChatGPT. But because in September 2025, we got access to something different. Agent skills that encode your judgment as reusable systems, not one-time prompts.

I wrote a long article explaining what changed, why it matters, and what resource-constrained legal teams should do about it.

**You'll get three things:**

**1. A working NDA review skill** you can copy and use today. No coding required. If you can write a checklist, you can build a skill.

**2. The economics breakdown.** Vendor tools charge $50/user/month ($600/year for solo counsel, $6,000/year for a 10-person team). Custom skills cost $20-50/month regardless of team size. I break down when each makes sense.

**3. A decision framework** to evaluate whether this approach fits your practice. Chat tools vs. vendor tools vs. custom skills—when to use each one.

[Read: Lawyers Got Prompt Engineering Wrong (And Why That Matters) →]

Maybe you don't need to learn what everyone else is learning. Maybe your instinct that "this doesn't solve my real problems" was right all along.

---

*This newsletter supports the full article "Lawyers Got Prompt Engineering Wrong (And Why That Matters)" published at alt-counsel.com.*
